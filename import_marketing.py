import os
import re
import unicodedata
import pandas as pd  
from docx import Document
from sqlalchemy import create_engine, text

# 1. è³‡æ–™åº«é€£ç·š
DB_URL = "postgresql://postgres:postgres@localhost:5432/bubble_tea"
engine = create_engine(DB_URL)

# 2. å®šç¾© 10 é–“å“ç‰Œ
STORE_NAMES = [
    "åŠŸå¤«èŒ¶", "å¤§èŒ—æœ¬ä½åˆ¶èŒ¶å ‚", "å¾—æ­£", "å…ˆå–é“", "æ¸…æ–°ç¦å…¨",
    "è¿·å…‹å¤", "Comebuy", "é¾œè¨˜", "äº”ååµ", "Cocoéƒ½å¯"
]

# å»ºç«‹å“ç‰Œèˆ‡è³‡æ–™åº«å“ç‰Œçš„å°ç…§è¡¨
BRAND_MAP = {
    "åŠŸå¤«èŒ¶": "åŠŸå¤«èŒ¶",
    "å¤§èŒ—": "å¤§èŒ—æœ¬ä½åˆ¶èŒ¶å ‚",
    "å¾—æ­£": "å¾—æ­£",
    "å…ˆå–é“": "å…ˆå–é“",
    "æ¸…å¿ƒç¦å…¨": "æ¸…æ–°ç¦å…¨", 
    "æ¸…æ–°ç¦å…¨": "æ¸…æ–°ç¦å…¨",
    "è¿·å®¢å¤": "è¿·å…‹å¤",  
    "è¿·å…‹å¤": "è¿·å…‹å¤",
    "comebuy": "Comebuy",
    "é¾œè¨˜": "é¾œè¨˜",
    "50åµ": "äº”ååµ",
    "cocoéƒ½å¯": "Cocoéƒ½å¯"
}

def load_menu_dict(file_path):
    """è®€å– Excelï¼Œå»ºç«‹èœå–®å­—å…¸"""
    menu = {}
    if not os.path.exists(file_path):
        print(f"âš ï¸ æ‰¾ä¸åˆ°èœå–®æª”æ¡ˆ: {file_path}ï¼Œå°‡ç„¡æ³•ç²¾æº–å°é½Šé£²æ–™åç¨±ã€‚")
        return menu

    try:
        df = pd.read_excel(file_path)
        df = df.fillna('')
        
        for index, row in df.iterrows():
            excel_brand = str(row.get('brand', '')).strip()
            item_name = str(row.get('item_name', '')).strip()
            
            if not excel_brand or not item_name: 
                continue
            
            db_brand = BRAND_MAP.get(excel_brand, excel_brand)
            if db_brand not in menu:
                menu[db_brand] = []
            menu[db_brand].append(item_name)
            
        print(f"âœ… æˆåŠŸè¼‰å…¥ {len(menu)} å®¶å“ç‰Œçš„èœå–®å­—å…¸ï¼")
    except Exception as e:
        print(f"âŒ è®€å– Excel ç™¼ç”ŸéŒ¯èª¤: {e}")
        
    return menu

def parse_docx_with_menu(file_path, platform, store_mapping, menu_dict):
    if not os.path.exists(file_path): return []
    doc = Document(file_path)
    posts, current_store_id, current_post_data = [], None, None
    header_pattern = re.compile(r'(\d{4}[-/\.]\d{1,2}[-/\.]\d{1,2}).*?(æŒ‰è®šæ•¸|è®š|Like)[ï¼š:\s]*(\d+)', re.IGNORECASE)

    id_to_store_name = {v: k for k, v in store_mapping.items()}

    for para in doc.paragraphs:
        line = para.text.strip()
        if not line: continue
        
        is_store = False
        for name, sid in store_mapping.items():
            if name in line and len(line) < 20:
                current_store_id, is_store = sid, True
                break
        if is_store: continue

        norm_line = unicodedata.normalize('NFKC', line)
        match = header_pattern.search(norm_line)
        
        if match:
            if current_post_data: posts.append(current_post_data)
            current_post_data = {
                "store_id": current_store_id, "platform": platform,
                "created_at": match.group(1).replace('/', '-').replace('.', '-'),
                "Like": int(match.group(3)), "lines": []
            }
        elif current_post_data:
            current_post_data['lines'].append(line)

    if current_post_data: posts.append(current_post_data)
    
    final_results = []
    
    # ğŸŒŸ çµ±ä¸€é›†ä¸­ç®¡ç†é»‘ç™½åå–®
    black_list = [
        'è²·ä¸€é€ä¸€', 'å„ªæƒ ', 'æŠ½ç', 'æ¨è–¦', 'æ–°å“', 'é™å®š', 'æ‰‹æ–', 'é£²æ–™', 
        'ç¾é£Ÿ', 'å°åŒ—', 'å°ä¸­', 'é«˜é›„', 'å°ç£', 'æ‰“å¡', 'å¥½å–', 'å–èµ·ä¾†', 
        'é–€å¸‚', 'æ´»å‹•', 'å¿«æ¨‚', 'ç¯€', 'è¯å', 'ä¸Šå¸‚', 'é–‹è³£', 'å¤–é€', 'äººæ°£',
        'å¿…å–', 'èœå–®', 'åŠ ç¢¼', 'æ—¥å¸¸', 'æ»‹å‘³', 'å£æ„Ÿ', 'ç³»åˆ—', 'è©¦è³£', 'å°ˆå±¬',
        'æ–°ä¸Šå¸‚', 'å›æ­¸', 'æ¯'
    ]
    # æ“´å……äº† 'è˜‹' èˆ‡ 'é»‘ç³–'
    white_list = [
        'èŒ¶', 'å¥¶', 'ç´…', 'ç¶ ', 'é’', 'çƒé¾', 'æ‹¿éµ', 'å¤šå¤š', 'å†°æ²™', 
        'æœ', 'æª¸æª¬', 'çç ', 'æ³¢éœ¸', 'ç²‰ç²¿', 'è•éº¥', 'æ˜¥', 'éµè§€éŸ³', 'æ±', 'å†°', 'å‡',
        'å¥¶è“‹', 'æ­è•¾', 'ç”˜éœ²', 'ç‘ªå¥‡æœµ', 'å†°èŒ¶', 'ç‰¹èª¿', 'èŠèŠ', 'é›™Q', 'æ¤°æœ', 'å¯¶',
        'è˜‹', 'é»‘ç³–' 
    ]

    for p in posts:
        full_text = "\n".join(p['lines'])
        
        # ğŸ§¹ é­”æ³•æ·¨åŒ–ï¼šåœ¨ä¸€é–‹å§‹å°±æŠŠæ‰€æœ‰éš±å½¢å­—å…ƒè·Ÿç‰¹æ®Šç©ºç™½å…¨éƒ¨æ®ºæ‰
        full_text = full_text.replace('\u200b', '').replace('\xa0', '').replace('\u200e', '')
        
        if not full_text.strip():
            continue
            
        matched_product = None
        store_name = id_to_store_name.get(p['store_id'])
        
        # ğŸ¯ ç¬¬ä¸€é—œï¼šExcel æ ¸å¿ƒå°é½Š
        if store_name in menu_dict:
            sorted_menu = sorted(menu_dict[store_name], key=len, reverse=True)
            for prod in sorted_menu:
                if prod.replace(" ", "") in full_text.replace(" ", ""):
                    matched_product = prod
                    break
        
        # ğŸ›¡ï¸ ç¬¬äºŒé—œï¼šHashtag æ™ºèƒ½æŠ“å–
        if not matched_product:
            hashtags = re.findall(r'#([^\s#ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼šã€Œã€ã€ã€‘()]+)', full_text)
            for tag in hashtags:
                tag_clean = tag.replace('_', '').replace('-', '') 
                
                # âœ¨ å‰æ´‹è”¥é­”æ³•ï¼šæŠŠé»‘åå–®çš„å­—ã€Œåˆªé™¤ã€è€Œä¸æ˜¯ã€Œæ•´çµ„å ±å»¢ã€
                for black_word in black_list:
                    tag_clean = tag_clean.replace(black_word, '')
                    
                if 2 <= len(tag_clean) <= 12:
                    if any(white_word in tag_clean for white_word in white_list):
                        matched_product = tag_clean
                        break
                        
        # ğŸš€ ç¬¬ä¸‰é—œï¼šå…¨æ–‡è©å¡Šæš´åŠ›æƒæ (è™•ç†ä¸åŠ  Hashtag çš„è²¼æ–‡)
        if not matched_product:
            chunks = re.split(r'[^\w\u4e00-\u9fa5]+', full_text)
            for chunk in chunks:
                chunk = chunk.strip()
                
                # âœ¨ å‰æ´‹è”¥é­”æ³•ï¼šæŠŠé»‘åå–®çš„å­—ã€Œåˆªé™¤ã€
                for black_word in black_list:
                    chunk = chunk.replace(black_word, '')
                    
                if 2 <= len(chunk) <= 10:
                    if any(white_word in chunk for white_word in white_list):
                        matched_product = chunk
                        break

        # ğŸ·ï¸ ç¬¬å››é—œï¼šçœŸçš„æŠ“ä¸åˆ°ï¼Œæ‰æ˜¯æ—¥å¸¸å»¢æ–‡
        if not matched_product:
            matched_product = "å“ç‰Œæ—¥å¸¸/ç„¡ç‰¹å®šé£²å“"
            
        final_results.append({
            "store_id": p['store_id'], "platform": p['platform'],
            "product_name": matched_product, "final_text": full_text,
            "created_at": p['created_at'], "Like": p['Like']
        })
    return final_results

def main():
    print("ğŸš€ å•Ÿå‹•èœå–®ç²¾æº–å°é½ŠåŒ¯å…¥ç®¡ç·š...")
    EXCEL_FILENAME = "beverage_with_fruit_column.xlsx"
    menu_dict = load_menu_dict(EXCEL_FILENAME)

    with engine.begin() as conn:
        conn.execute(text("TRUNCATE TABLE store CASCADE;"))
        store_map = {name: i for i, name in enumerate(STORE_NAMES, 1)}
        for name, sid in store_map.items():
            conn.execute(text("INSERT INTO store (id, tenant_id, name, location_city) VALUES (:id, 1, :name, 'å°åŒ—å¸‚')"), {"id": sid, "name": name})
        
        conn.execute(text("TRUNCATE TABLE marketing_content RESTART IDENTITY CASCADE;"))
        for plat, file in [("FB", "é£²æ–™æ–‡æ¡ˆfb.docx"), ("IG", "é£²æ–™æ–‡æ¡ˆig.docx")]:
            for p in parse_docx_with_menu(file, plat, store_map, menu_dict):
                conn.execute(text("""INSERT INTO marketing_content (store_id, platform, product_name, final_text, created_at, "Like")
                                     VALUES (:store_id, :platform, :product_name, :final_text, CAST(:created_at AS TIMESTAMP), :Like)"""), p)
    print("ğŸ‰ è³‡æ–™æ¸…æ´—åŒ¯å…¥å®Œæˆï¼ç”¢å“åç¨±å·²å®Œç¾å°é½Šã€‚")

if __name__ == "__main__":
    main()